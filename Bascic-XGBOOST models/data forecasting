import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
import shap
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from scipy import stats

# Set matplotlib fonts - fix display issues
try:
    plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial']
    plt.rcParams['axes.unicode_minus'] = False
except:
    # If SimHei is not available, use system default fonts
    plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial']
    plt.rcParams['axes.unicode_minus'] = False


def add_noise(X, noise_factor=0.05):
    noise = np.random.normal(loc=0.0, scale=noise_factor, size=X.shape)
    X_noisy = X + noise
    return X_noisy


def remove_data_by_indices(data, indices_to_remove):
    mask = np.ones(len(data), dtype=bool)
    mask[indices_to_remove] = True

    if isinstance(data, pd.DataFrame):
        filtered_data = data[mask]
    elif isinstance(data, np.ndarray):
        filtered_data = data[mask]
    elif isinstance(data, list):
        filtered_data = [data[i] for i in range(len(data)) if i not in indices_to_remove]
    else:
        raise ValueError("Unsupported data type. Please use NumPy arrays, Pandas DataFrame, or lists.")

    return filtered_data


if __name__ == '__main__':
    # Set fixed random seed to ensure reproducibility
    random_state = 42
    np.random.seed(random_state)  # Set random seed

    # File path - fix encoding issues
    file_path = r"Supporting Input Dataset.csv"

    # Try different encodings to read the file
    encodings = ['utf-8', 'gbk', 'gb2312', 'latin1', 'cp1252']
    data = None

    for encoding in encodings:
        try:
            data = pd.read_csv(file_path, encoding=encoding)
            print(f"Successfully read file with {encoding} encoding")
            break
        except UnicodeDecodeError:
            print(f"{encoding} encoding failed, trying next...")
            continue
        except Exception as e:
            print(f"{encoding} encoding error: {e}")
            continue

    if data is None:
        raise ValueError("Unable to read file, please check file path and encoding")

    # Print first few rows and data dimensions to ensure correct loading
    print("First few rows of data:")
    print(data.head())
    print("Data dimensions:", data.shape)

    # Check number of data columns
    if data.shape[1] != 16:
        raise ValueError("Number of data columns does not meet expectations. Should have 16 columns!")

    # Select features and target variables (assuming last column is target, others are features)
    X = data.iloc[:, :-1].values
    y = data.iloc[:, -1].values

    # === Scale the first column by 100 times ===
    X[:, 0] = X[:, 0] * 100.0

    # Remove outliers (using Z-score) - only process features
    z_scores = np.abs(stats.zscore(X))
    threshold = 3
    mask = (z_scores < threshold).all(axis=1)
    X_cleaned = X[mask]
    y_cleaned = y[mask]

    # Select 20% of data as test data, will be removed after augmentation
    len_test = int(len(X_cleaned) * 0.2)
    # Randomly generate indices
    test_indices = np.random.choice(len(X_cleaned), size=len_test, replace=False)

    X_augmented_list = [X_cleaned]
    y_augmented_list = [y_cleaned]
    # Add noise for sample augmentation until data volume reaches 1 times original data
    expansion_factor = 1
    for _ in range(expansion_factor - 1):
        X_noisy = add_noise(X_cleaned, noise_factor=0.05)
        X_augmented_list.append(X_noisy)
        y_augmented_list.append(y_cleaned)

    # Merge original and augmented data
    X_augmented = np.vstack(X_augmented_list)
    y_augmented = np.hstack(y_augmented_list)

    # Fill missing values with median
    median_values = np.nanmedian(X_augmented, axis=0)
    nan_mask = np.isnan(X_augmented)
    X_augmented[nan_mask] = np.take(median_values, np.where(nan_mask)[1])

    # ----------------- Modified to use range_values with zero division protection -----------------
    # Calculate min/max and range for each column
    min_values = np.min(X_augmented, axis=0)
    max_values = np.max(X_augmented, axis=0)
    range_values = max_values - min_values
    # Prevent division by zero when max == min; set range to 1 for columns with zero range (won't change normalization result)
    zero_range = range_values == 0
    if zero_range.any():
        range_values[zero_range] = 1.0
    # Complete normalization using range_values
    X_normalized = (X_augmented - min_values) / range_values
    # -------------------------------------------------------------------------------

    # y_augmented remains unchanged, no offset processing!

    # Manually select unaugmented data as test set
    X_test, y_test = X_normalized[test_indices], y_augmented[test_indices]
    X_train = remove_data_by_indices(X_normalized, test_indices)
    y_train = remove_data_by_indices(y_augmented, test_indices)

    # XGBoost model training
    model = xgb.XGBRegressor(
        objective='reg:squarederror',
        random_state=random_state,
        max_depth=3,  # Pre-pruning parameter
        min_child_weight=1,  # Pre-pruning parameter
        gamma=0.1,  # Pre-pruning parameter
        reg_lambda=0.3,  # L2 regularization
        reg_alpha=0.4  # L1 regularization
    )
    best_model = model.fit(X_train, y_train)
    y_pred_train = best_model.predict(X_train)
    y_pred_test = best_model.predict(X_test)

    # Calculate evaluation metrics
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)
    mse_test = mean_squared_error(y_test, y_pred_test)
    mae_test = mean_absolute_error(y_test, y_pred_test)

    # Print evaluation results
    print("\nTraining set evaluation results:")
    print(f"R² (training set): {train_r2:.4f}")
    print("\nTest set evaluation results:")
    print(f"R² (test set): {test_r2:.4f}")
    print(f"MSE (test set): {mse_test:.4f}")
    print(f"MAE (test set): {mae_test:.4f}")

    # Set global font to Arial to avoid display issues
    plt.rcParams['font.family'] = 'Arial'

    # Create figure framework
    plt.figure(figsize=(12, 10))
    g = sns.JointGrid(height=8, ratio=5, space=0)

    # Draw scatter plot (with edge colors)
    scatter_train = g.ax_joint.scatter(y_train, y_pred_train, color='blue', edgecolor='white',
                                       alpha=0.7, s=60, linewidth=0.5)
    scatter_test = g.ax_joint.scatter(y_test, y_pred_test, color='green', edgecolor='white',
                                      alpha=0.7, s=60, linewidth=0.5)

    # Draw diagonal line
    min_val = min(min(y_train), min(y_test))
    max_val = max(max(y_train), max(y_test))
    g.ax_joint.plot([min_val, max_val], [min_val, max_val],
                    color='red', linestyle='--', linewidth=1.5, label="Ideal line")

    # Add R² text annotation (using English to avoid font issues)
    text_str = f'Train R² = {train_r2:.3f}\nTest R² = {test_r2:.3f}'
    g.ax_joint.text(0.05, 0.95, text_str, transform=g.ax_joint.transAxes,
                    fontsize=26, fontname='Arial',
                    verticalalignment='top')

    # Set axis labels (using English)
    g.ax_joint.set_xlabel('True Values', fontsize=20, fontname='Arial')
    g.ax_joint.set_ylabel('Predicted Values', fontsize=20, fontname='Arial')

    # Set axis tick font
    g.ax_joint.tick_params(axis='both', which='major', labelsize=18)

    # Add top histogram
    hist_true = sns.histplot(x=y_train, ax=g.ax_marg_x, color='blue', alpha=0.5,
                             kde=True, linewidth=0)
    sns.histplot(x=y_test, ax=g.ax_marg_x, color='green', alpha=0.5,
                 kde=True, linewidth=0)

    # Add right histogram
    hist_pred = sns.histplot(y=y_pred_train, ax=g.ax_marg_y, color='blue', alpha=0.5,
                             kde=True, linewidth=0)
    sns.histplot(y=y_pred_test, ax=g.ax_marg_y, color='green', alpha=0.5,
                 kde=True, linewidth=0)

    # Set graph style
    for ax in [g.ax_joint, g.ax_marg_x, g.ax_marg_y]:
        # Set borders
        for spine in ax.spines.values():
            spine.set_color('black')
            spine.set_linewidth(1.5)
        # Remove grid
        ax.grid(False)
        # Set tick label font
        ax.tick_params(labelsize=22)

    # Set histogram background colors
    g.ax_marg_x.set_facecolor('white')
    g.ax_marg_y.set_facecolor('white')

    # Adjust histogram positions
    g.ax_marg_x.set_position([g.ax_joint.get_position().x0,
                              g.ax_joint.get_position().y1 + 0.01,
                              g.ax_joint.get_position().width,
                              0.15])
    g.ax_marg_y.set_position([g.ax_joint.get_position().x1 + 0.01,
                              g.ax_joint.get_position().y0,
                              0.15,
                              g.ax_joint.get_position().height])

    # Add legend (using English)
    legend_elements = [
        plt.Line2D([0], [0], marker='o', color='w', label=f'Train set',
                   markerfacecolor='blue', markersize=10),
        plt.Line2D([0], [0], marker='o', color='w', label=f'Test set',
                   markerfacecolor='green', markersize=10),
        plt.Line2D([0], [0], color='red', linestyle='--', label='Ideal line')
    ]
    legend = g.ax_joint.legend(handles=legend_elements, loc='lower right',
                               framealpha=0.9, prop={'family': 'Arial', 'size': 16})

    plt.tight_layout()
    plt.show()

    # ------------------ New: Predict new data and save to desktop ------------------
    # Please change new_data_path to the new data file (CSV) to be predicted, which should only contain feature columns (number of columns must match training)
    new_data_path = r"C:\Users\Administrator\Desktop\支撑数据 - 副本\out2.csv"
    if os.path.exists(new_data_path):
        new_df = pd.read_csv(new_data_path)
        # New data column count must match training feature count (training feature count = original data columns - 1)
        n_features = data.shape[1] - 1
        if new_df.shape[1] < n_features:
            raise ValueError(f"New data column count {new_df.shape[1]} is less than training feature count {n_features}.")
        elif new_df.shape[1] > n_features:
            # If new data contains extra columns (e.g., with label column or index), only take first n_features columns
            new_df = new_df.iloc[:, :n_features]

        # Convert to numpy and follow same preprocessing pipeline as before: fill missing (using training set median_values) -> normalize (using training min/range)
        new_X = new_df.values.astype(float)
        # Fill missing values
        nan_mask_new = np.isnan(new_X)
        if nan_mask_new.any():
            new_X[nan_mask_new] = np.take(median_values, np.where(nan_mask_new)[1])

        # Normalization: Use training phase min_values and range_values
        new_X_normalized = (new_X - min_values) / range_values

        # Diagnostics: Print new data normalization range (first 3 columns)
        print("New samples after normalization (first 3 cols) min:", np.round(new_X_normalized.min(axis=0)[:3], 6))
        print("New samples after normalization (first 3 cols) max:", np.round(new_X_normalized.max(axis=0)[:3], 6))

        # Make predictions
        preds = best_model.predict(new_X_normalized)

        # Diagnostics: Print some prediction comparisons
        print("Example: y_train[:6] =", np.round(y_train[:6], 6))
        print("Example: preds[:6]   =", np.round(preds[:6], 6))

        # Save predictions to desktop
        desktop_path = os.path.join(os.path.expanduser("~"), "Desktop")
        save_name = "predictions_from_model.csv"
        save_path = os.path.join(desktop_path, save_name)

        out_df = pd.DataFrame(new_df)  # Keep original new data (may have been truncated)
        out_df['prediction'] = preds
        out_df.to_csv(save_path, index=False, encoding='utf-8-sig')
        print(f"Predictions saved to: {save_path}")
    else:
        print(f"New data file not found: {new_data_path}. To predict new samples, please place CSV at this path or modify new_data_path variable.")
