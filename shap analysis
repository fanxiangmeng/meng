# Create SHAP explainer
explainer = shap.Explainer(best_model)

# Calculate SHAP values (Note: use training data)
shap_values = explainer(X_train)

# SHAP bar plot (color changed to #7F7FFF)
plt.figure(figsize=(10, 8))
# Note: shap.plots.bar in some shap versions doesn't support color parameter, so use summary_plot(plot_type='bar') with color specification
try:
    shap.plots.bar(shap_values, max_display=10)
except TypeError:
    # Fall back to bar mode of summary_plot with specified color
    shap.summary_plot(shap_values, features=X_train, feature_names=[f'Feature {i + 1}' for i in range(X_train.shape[1])],
                      plot_type="bar", color="#7F7FFF")
else:
    # If shap.plots.bar succeeds, manually draw single-color bar plot to ensure consistent color (compatibility protection)
    import numpy as _np
    mean_abs_shap = _np.abs(shap_values.values).mean(axis=0)
    feat_names = [f'Feature {i + 1}' for i in range(X_train.shape[1])]
    sorted_idx = _np.argsort(mean_abs_shap)[::-1]
    top_n = min(10, len(feat_names))
    plt.barh([feat_names[i] for i in sorted_idx[:top_n]][::-1],
             mean_abs_shap[sorted_idx[:top_n]][::-1],
             color="#7F7FFF")
    plt.gca().invert_yaxis()
    plt.title("Feature Importance based on SHAP", fontsize=18, fontname="Arial")
    plt.xlabel("Mean |SHAP value|", fontsize=14, fontname="Arial")
    plt.tight_layout()
    plt.show()

# Draw SHAP summary plot (with directionality)
shap.summary_plot(shap_values, features=X_train,
                  feature_names=[f'Feature {i + 1}' for i in range(X_train.shape[1])])
plt.show()
